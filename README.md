RCPA WGS PTP Bioinformatics Pipeline
====================================

-   [RCPA WGS PTP Bioinformatics
    Pipeline](#rcpa-wgs-ptp-bioinformatics-pipeline)
    -   [Background](#background)
    -   [Running the pipeline](#running-the-pipeline)
        -   [Software Dependencies](#software-dependencies)
        -   [Hardware requirements](#hardware-requirements)
        -   [Clone the repo](#clone-the-repo)
        -   [Running on Mac](#running-on-mac)
        -   [Running on Unix](#running-on-unix)
        -   [Running on Windows](#running-on-windows)
    -   [Running with your own data](#running-with-your-own-data)
    -   [The pipeline](#the-pipeline)
        -   [Bioinformatics pipeline](#bioinformatics-pipeline)
        -   [Tools](#tools)
        -   [The output](#the-output)
    -   [Questions or issues?](#questions-or-issues)
    -   [Acknowledgments](#acknowledgments)
    -   [Authors](#authors)
    -   [Maintainer](#maintainer)
    -   [References](#references)

Background
----------

In this repository, we document and share the Bioinformatics pipeline
used to analyse FASTQ data generated by Australian public health
laboratories participating in the RCPA’s whole-genome sequencing (WGS)
proficiency testing program (PTP).

Running the pipeline
--------------------

### Software Dependencies

-   Python \>= 3.6
-   Snakemake \>= 5.4
-   Singularity \>=3.1

### Hardware requirements

-   8 GB of RAM (16 GB is recommended if running within a Virtual
    Machine)
-   30 GB of Hard Disk space

### Clone the repo

Clone the repository:

``` console
git clone https://github.com/cdgn-anz/wgs-ptp-pipeline
```

### Running on Mac

Once you cloned the repository, the basic commands to get up and running
are the following (below we describe how to get your machine setup if
you don’t have `vagrant` and `virtualbox` installed):

``` bash
cd rcpa_wgs_ptp
vagrant up
vagrant ssh
cd qc_folder && snakemake --use-singularity
```

To install `vagrant` and `virtualbox`. I suggest using `brew`.

For instructions on how to install `brew`, please watch this
[video](https://youtu.be/31eTw5xRHBA).

Once `brew` is installed, you can install `vagrant` and `virtualbox`
with the following command in your Terminal:

1.  `brew cask install vagrant virtualbox`

Then install the `vagrant-disksize` plug-in with the following command:

1.  `vagrant plugin install vagrant-disksize`

Now you are ready to get started.

First, bootstrap the virtual machine (VM) with the following command:

1.  `vagrant up` — will bootstrap the Ubuntu box with Singularity
    installed, and will download all the Singularity images.

Second, login to the VM with the following command:

1.  `vagrant ssh` — will log you on to the Ubuntu virtual machine.

Third, run the test suite with the following command:

1.  `cd qc_folder && snakemake --use-singularity` — will run the test
    dataset

### Running on Unix

Once you cloned the repository, the basic commands to get up and running
are the following (below we describe how to get your machine setup if
you don’t have `vagrant` and `virtualbox` installed):

``` bash
cd rcpa_wgs_ptp
vagrant up
vagrant ssh
cd qc_folder && snakemake --use-singularity
```

To get `vagrant` and `virtualbox` installed in your unix system, please
follow the instructions for `1` through `5` for the `Mac` above to
install `brew` and then to install the two applications, and run the
test suite.

### Running on Windows

**NOTE**: This has not been tested, please let us know how it goes.

Once you cloned the repository, the basic commands to get up and running
are the following (below we describe how to get your machine setup if
you don’t have `vagrant` and `virtualbox` installed):

To install `vagrant` and `virtualbox` on Windows machine, please watch
this [video](https://youtu.be/mPBWWu7sZU4). Here, we provide the
`Vagrantfile`, so you do not need to worry about Vagrant boxes.

Once `vagrant` and `virtualbox` is installed, you can follow from
command `2.` from the `Mac` instructions above.

Running with your own data
--------------------------

If you are in the VM, exit it by typing `exit` in the terminal.

The first step is to put the data in a place that is accessible to the
VM. The easiest way is to create a data folder in the `rcpa_wgs_ptp`
folder, and copy the FASTQ data to that folder.

Then, you need to create an `input.tab` file with the following header
in the `data` folder:

| SAMPLE\_ID |               R1               |               R2               | SAMPLE\_TYPE |
|:----------:|:------------------------------:|:------------------------------:|:------------:|
|     id1    | /vagrant/data/id1\_R1.fastq.gz | /vagrant/data/id1\_R2.fastq.gz |     data     |
|    ntc1    | /vagrant/data/id1\_R1.fastq.gz | /vagrant/data/id1\_R2.fastq.gz |      ntc     |

`SAMPLE_TYPE` can either be `data` or `ntc`. `R1` and `R2` are the full
path within the VM to the files. Notice that the path is
`/vagrant/data`, that is because `vagrant` will mirror the folder that
contains the `Vagrantfile` (in this case the folder within which you
cloned the repository and created the `data` folder) to the path
`/vagrant` inside the VM.

Make sure you are in the folder where you cloned the repository to, and
jump back into the VM with `vagrant ssh`.

Type the following:

``` bash
cd qc_folder
rm -f *.toml
snakemake --config input="/vagrant/data/input.tab" --use-singularity
```

If everything works out, you should have a `qc_results.toml` file. An
example is output file is provided in the repo:
[qc\_results.toml](./qc_results.toml)

The pipeline
------------

### Bioinformatics pipeline

The bioinformatics pipeline was implemented using Snakemake (Köster and
Rahmann 2012), a bioinformatics pipeline engine. All the bioinformatic
tools were installed in Singularity containers (Kurtzer, Sochat, and
Bauer 2017), ensuring the reproducibility of the pipeline by others. All
tools were installed using the package manager Conda
(https://docs.conda.io/en/latest/), and both version and build were
specified in order to aid in reproducibility. All containers are stored
in CloudStor and available for download (see Containers section below).
Finally, the whole process can be reproduced by using `vagrant`
(https://www.vagrantup.com/) and `virtualbox`
(https://www.virtualbox.org/) following the instructions available on
the GitHub repository
(https://www.github.com/cdgn-anz/wgs-ptp-pipeline). The repository
contains all the code for the pipelines and recipes for building the
Singularity containers.

Two Snakemake pipelines were written named: data and ntc. The first
processed the FASTQ data associated with each sample, and the second
processed the samples labelled as “negative control.” Both used the same
elements.

The data pipeline consisted of the following steps:

1.  Raw read assessment: generating summary statistics for the raw
    reads.
2.  Read trimming: removing sequencing adapter sequences, and regions of
    low quality from reads.
3.  Trimmed read assessment: generating summary statistics for the
    trimmed reads.
4.  Species identification: using kmers to identify the most likely
    (predominant) species in the sample.
5.  Genome size and coverage estimate: using kmer counting to estimate
    the genome size and coverage (redundancy).
6.  Assembling trimmed reads: generating a draft genome from the reads.
7.  Assembled read assessment: generating summary statistics about the
    draft genome.
8.  Sequence typing: generating an MLST result based on the draft genome
    assembly (if an appropriate scheme exists for the organism).
9.  AMR detection: detecting any AMR genes based on the draft genome
    assembly.
10. Serotyping: inferring the likely serotype of the organism based on
    draft genome assembly (if an appropriate *in silico* typing tool
    exists for the organism detected in step 4, above)

The ntc pipeline was composed of steps one and four above. The goal
being to ensure the number of reads in the negative control is low, and
detecting any potential contaminating species.

### Tools

A combination of tools was used to form the pipeline (detailed below).
Read statistics were generated with `seqtk` (Li 2018); reads were
trimmed with `trimmomatic` (Bolger, Lohse, and Usadel 2014) using a
database of Illumina adapter sequences; species identification was done
using `kraken2` (Wood and Salzberg 2014) using the MiniKraken Database
v2 (includes bacteria, Archaea, virus, and human genomes) released on
2018-11-01; genome size and coverage estimation from kmers was done
using `mash` (Ondov et al. 2016); read assembly was done using `shovill`
(Seemann et al. 2018) using the `spades` assembler (Bankevich et al.
2012); assessment of the draft assembly was done with `quast` (Gurevich
et al. 2013); MLST was inferred using `mlst` (Seemann 2018b) using
schemes available on PubMLST (Jolley, Bray, and Maiden 2018); presence
of AMR genes was inferred using `abricate` (Seemann 2018a) using the
NCBI AMR reference database (NCBI, n.d.); and serotype was inferred
using `sistr` (Yoshida et al. 2016), a tool for inferring the serotype
of *Salmonella enterica*.

1.  Read stats
    1.  `seqtk`
2.  Read trimming
    1.  `timmomatic`
3.  Estimating genome size based on kmers
    1.  `mash`
4.  Species identification
    1.  `kraken2` — for the purposes of the WGS PTP we are only using
        the 8GB minikraken DB.
5.  Assembly
    1.  `shovill` — using `spades 3.13.0`
6.  Sequence Typing
    1.  `mlst` — using schemes available on PubMLST
7.  Serotyping
    1.  `SISTR` for *Salmonella*
8.  AMR detection
    1.  `abricate` — using the NCBI database

### The output

The pipeline will generate a `qc_results.toml` file with all the data
generated during the running of the pipeline. This file can be loaded
into `R`:

``` r
install.packages("RcppTOML")
data <- RcppTOML::parseTOML(input = "qc_results.toml")
names(data) # will show you that data is a list of lists, with the top 
            # elements named after the analysed samples
```

It can also be loaded in to `python`:

First make sure to install the `PyTOML` package:

``` bash
pip3 install pytoml
```

Then in `python`:

``` python
import toml
data = toml.load("qc_results.toml") # retuns a dictionary
print(data.keys())
```

Additional tools to interact with the data will be provided soon.

Questions or issues?
--------------------

Please post your queries, suggestions, comments, bug reports to
[Issues](https://github.com/cdgb-anz/rcpa-wgs-ptp/issues)

Acknowledgments
---------------

We would like to acknowledge the CDGN Bioinformatics Working group for
helping develop the minimum criteria used for assessing the data.

Authors
-------

-   Anders Gonçalves da Silva - MDU PHL
-   Torsten Seemann - MDU PHL and Doherty Institute for Infection and
    Immunity
-   Susan Ballard - MDU PHL
-   Torsten Theis – RCPA
-   Joanna Gray - RCPA
-   Katherine Lau - RCPA

Maintainer
----------

-   Anders Gonçalves da Silva (andersgs at gmail dot com)

References [references]
----------

<div id="refs" class="references" markdown="1">

<div id="ref-Bankevich2012-of" markdown="1">

Bankevich, Anton, Sergey Nurk, Dmitry Antipov, Alexey A Gurevich,
Mikhail Dvorkin, Alexander S Kulikov, Valery M Lesin, et al. 2012.
“SPAdes: A New Genome Assembly Algorithm and Its Applications to
Single-Cell Sequencing.” *J. Comput. Biol.* 19 (5): 455–77.

</div>

<div id="ref-Bolger2014-qe" markdown="1">

Bolger, Anthony M, Marc Lohse, and Bjoern Usadel. 2014. “Trimmomatic: A
Flexible Trimmer for Illumina Sequence Data.” *Bioinformatics* 30 (15):
2114–20.

</div>

<div id="ref-Gurevich2013-cw" markdown="1">

Gurevich, Alexey, Vladislav Saveliev, Nikolay Vyahhi, and Glenn Tesler.
2013. “QUAST: Quality Assessment Tool for Genome Assemblies.”
*Bioinformatics* 29 (8): 1072–5.

</div>

<div id="ref-Jolley2018-jn" markdown="1">

Jolley, Keith A, James E Bray, and Martin C J Maiden. 2018. “Open-Access
Bacterial Population Genomics: BIGSdb Software, the PubMLST.org Website
and Their Applications.” *Wellcome Open Res* 3 (September): 124.

</div>

<div id="ref-Koster2012-cf" markdown="1">

Köster, Johannes, and Sven Rahmann. 2012. “Snakemake–a Scalable
Bioinformatics Workflow Engine.” *Bioinformatics* 28 (19): 2520–2.

</div>

<div id="ref-Kurtzer2017-se" markdown="1">

Kurtzer, Gregory M, Vanessa Sochat, and Michael W Bauer. 2017.
“Singularity: Scientific Containers for Mobility of Compute.” *PLoS One*
12 (5): e0177459.

</div>

<div id="ref-Li2018-ow" markdown="1">

Li, Heng. 2018. “Seqtk: Toolkit for Processing Sequences in FASTA/Q
Formats.” <https://github.com/lh3/seqtk>.

</div>

<div id="ref-Ncbi_undated-gz" markdown="1">

NCBI. n.d. “NCBI AMR Reference Gene Database.”
<https://www.ncbi.nlm.nih.gov/pathogens/isolates#/refgene/>.

</div>

<div id="ref-Ondov2016-gn" markdown="1">

Ondov, Brian D, Todd J Treangen, Páll Melsted, Adam B Mallonee, Nicholas
H Bergman, Sergey Koren, and Adam M Phillippy. 2016. “Mash: Fast Genome
and Metagenome Distance Estimation Using MinHash.” *Genome Biol.* 17
(1): 132.

</div>

<div id="ref-Seemann2018-aa" markdown="1">

Seemann, Torsten. 2018a. “ABRicate: Mass Screening of Contigs for
Antimicrobial and Virulence Genes.”
<https://github.com/tseemann/abricate>.

</div>

<div id="ref-Seemann2018-yj" markdown="1">

———. 2018b. “Mlst: Scan Contig Files Against PubMLST Typing Schemes.”
<https://github.com/tseemann/mlst/>.

</div>

<div id="ref-Seemann2018-dk" markdown="1">

Seemann, Torsten, Jason Kwong, Simon Gladman, and Anders Gonçalves da
Silva. 2018. “Shovill: Faster SPAdes Assembly of Illumina Reads.”
<https://github.com/tseemann/shovill>.

</div>

<div id="ref-Wood2014-we" markdown="1">

Wood, Derrick E, and Steven L Salzberg. 2014. “Kraken: Ultrafast
Metagenomic Sequence Classification Using Exact Alignments.” *Genome
Biol.* 15 (3): R46.

</div>

<div id="ref-Yoshida2016-uu" markdown="1">

Yoshida, Catherine E, Peter Kruczkiewicz, Chad R Laing, Erika J Lingohr,
Victor P J Gannon, John H E Nash, and Eduardo N Taboada. 2016. “The
Salmonella in Silico Typing Resource (SISTR): An Open Web-Accessible
Tool for Rapidly Typing and Subtyping Draft Salmonella Genome
Assemblies.” *PLoS One* 11 (1): e0147101.

</div>

</div>
